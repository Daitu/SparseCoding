\subsection{Inference of Sparse code}

We will focus here on the $\gamma$ calculation during the sparse coding step of learning algorithm (i.e. part (1) of algorithm 1). There are different ways to obtain the sparse coefficients: Some based on the $l_0$ norm, other on the $l_1$ norm.

\subsubsection{$l_0$ norm based}
Matching pursuit algorithm proposed in \cite{258082} intended to approximately solve the basic pursuit problem which is normally unacceptable in terms of calculation (if $D$ is large):
\begin{center}
 $\min\limits_{\gamma_i} \|\underset{r}{\underbrace{ x_i - D \hspace{3px} \gamma_i }}\|_2^2 \ s.t. \ \|\gamma_i \|_0 \leq L$\\
 (with $r$ the residual)
\end{center}
Matching pursuit algorithm is a greedy algorithm which iteratively generates a sorted list of atom and weighting scalars that represent the sub-optimal solution. An improvement of this algorithm is called Orthogonal Matching Pursuit which Orthogonalizes all the chosen elements.
\begin{algorithm}
 \caption{Orthogonal Matching Pursuit Algorithm}
\begin{algorithmic} 
 \REQUIRE D, $x$
 \STATE $\Gamma = \O$
 \WHILE{$\|\gamma\|_0 < L$}
    \STATE \textit{/* Pick the element that most reduces the objective */}
    \STATE $e \leftarrow \underset{i \in \Gamma}{\argmin} \{ \underset{\gamma}{\min}\|x - D_{\Gamma \cup \{e\}} \gamma\|_2^2\}$ 
    \STATE \textit{/* Update the active set */}
    \STATE $\Gamma \leftarrow \Gamma \cup \{e\}$
    \STATE \textit{ /* Update $\gamma$ and the residual */}
    \STATE $\gamma \leftarrow (D_\Gamma^{\intercal} D_\Gamma)^{-1} D_\Gamma^{\intercal} x$
    \STATE $r \leftarrow x - D\gamma_\Gamma$
 \ENDWHILE
\end{algorithmic}
\end{algorithm}


\subsubsection{$l_1$ norm based}
With this relaxation of the $l_0$ norm, we could use a gradient descent method to solve this optimization problem:\\
\begin{center}
$\Delta_{\gamma} l(x^{(t)}) = D^{\intercal} (D \gamma - x) + \lambda sign(\gamma)$
\end{center}
However,  it's well known that $l_1$ norm is not differentiable at 0, in this case, if $\gamma$ changes sign because of $l_1$ norm gradient then clamp to 0:
\begin{center}
 Gradient descent step for each $k_th$ element of $\gamma$:\\\vspace{0.2cm}
 $\gamma_k = \gamma_k - \alpha (D_{.,k})^{\intercal} (D \gamma - x)$\\\vspace{0.3cm}
 Clamp to 0:\\ \vspace{0.2cm}
  if  sign($\gamma_k) \neq$ sign$(\gamma_k - \alpha \lambda$ sign$(\gamma_k))$ then: $\gamma_k = 0$\\
\indent else $\gamma_k = \gamma_k - \alpha \lambda$ sign$(\gamma_k)$
\end{center}
This algorithm is called Iterative Shrinkage and Thresholding Algorithm:
\begin{algorithm}
 \caption{ISTA (Iterative Shrinkage and Thresholding Algorithm)}
 \begin{algorithmic}
  \STATE initialize $\gamma$ 
  \WHILE{$\gamma$ has not converged}
    \FORALL { $\gamma_k$ \textbf{in} $\gamma$ }
        \STATE $\gamma_k = \gamma_k - \alpha (D[:,k])^{\intercal}  (D\gamma - x)$
        \STATE $\gamma_k = shrink(\gamma_k,\alpha\lambda)$
        \ENDFOR
    \ENDWHILE
\RETURN $\gamma$
\STATE 
\STATE Here \textbf{shrink(a,b) }= [..., sign$(a_i)$ max($|a_i| - b_i$, 0), ...]

 \end{algorithmic}

\end{algorithm}
% 
% The original problem is a combinatorial problem (proven to be NP-hard). To solve this problem we use relaxation methods (then we can smooth the $L_0$ and use continuous optimization techniques) or greedy methods (then build the solution one non-zero element at time).
% \subsubsection{Compute $\alpha$ }
% %Inderence of sparse code=============================================
% \paragraph{Idea}f
% Here we develop  step (1) of our algorithm.\\
% Assume we are given a dictionary matrix D, how do we compute $h(x^{(t)})$. We have to optimize:
% \begin{center}  Basic Pursuit:
% $l(x^{(t)}) = \frac{1}{2} \| x^{t}- D \alpha^{(t)} \|^{2}_{2} + \lambda \|\alpha^{(t)}\|_1 w.r.t. \alpha^{(t)}$\\ 
% \end{center} 
% Here we used relaxation method to switch from norm $l_0$ to $l_1$ know as the  Basic Pursuit (vs Matching Pursuit, a greedy method,  if we keep $l_0$ norm and find one atom at a time).\\
% We could use a gradient descent method to solve this minimization:\\
% \begin{center}
% $\Delta_{\alpha^{(t)}} l(x^{(t)}) = D^T (D \alpha^{(t)} - x^{(t)}) + \lambda sign(\alpha^{(t)})$
% \end{center}
% The issue is $l_1$ norm is not differentiable at 0. The solution is : if $\alpha^{(t)}$ changes sign because of $l_1$ norm gradient then clamp to 0.That mean :
% 
% $\alpha^{(t)}_k = \alpha^{(t)}_k   - \alpha (D_{., k})^T (D \alpha^{(t)} - x^{(t)})$\\
% \indent if  sign($\alpha^{(t)}_k) \neq$ sign$(\alpha^{(t)}_k - \alpha \lambda$ sign$(\alpha^{(t)}_k) )$ then: $\alpha^{(t)}_k = 0$\\
% \indent else $\alpha^{(t)}_k = \alpha^{(t)}_k - \alpha \lambda$ sign$(\alpha^{(t)}_k)$
% \paragraph{ISTA (Iterative Shrinkage and Thresholding Algorithm)}
% :
% \begin{lstlisting}[language=Python,frame=single]
% initialize h 
% while h not_converged:
%     for each h_k in h:
%         h_k = h_k -alpha * transpose(D[:,k]) * (D*h - x)
%         h_k = shrink(h_k,alpha*lambda_coef)
% return h
% \end{lstlisting}
% Here \textbf{shrink(a,b) }= [..., sign$(a_i)$ max($|a_i| - b_i$, 0), ...]\\
