\subsection{Supervised Dictionary learning LC-KSVD}
\cite{6516503} propose a supervised learning algorithm to learn a compact and discriminative dictionary for sparse coding using a new label consistency constraint ( ``discriminative sparse-code error") and combined reconstruction error and classification error to form a unified objective function which can be solved with the K-SVD algorithm.
\subsubsection{Dictionary Learning for classification}
A good classifier $f(x)$ can be obtained by determining its model parameters $W \in R^{m \times K} $ satisfying:
\begin{center}
 $W = \underset{W}{\argmin} \underset{i}{\sum}\mathcal{L}\{h_i,f(x_i,W)\} + \lambda_1 \|W\|^2_F$
\end{center}
where: $m$ is the number of classes, $\mathcal{L}$ is the classification loss function (typically logistic loss function, suqare hinge loss, simple quadratic loss function), $h_i$ the label of $y_i$ and $\lambda_1$ a regularization parameters (which prevents overfitting).\\
The objective function for learning $D$ and $W$ jointly can be :
\begin{center}
 $<D,W,\alpha> = \underset{D,W,\alpha}{\argmin} \|X -D\alpha\|^2_2 + \underset{i}{\sum}\mathcal{L}\{h_i,f(\alpha_i,W)\} + \lambda_1 \|W\|^2_F$  s.t.  $\forall i, \|\alpha_i\|_0 \leq T$ 
\end{center}

The sparse code $\alpha_i$ can be used as a feature descriptor of input signal $x_i$, then the risk minimization formulation can be written as:
\begin{center}
 $<D,W> = \underset{D,W}{\argmin}\underset{i}{\sum}\mathcal{L}\{h_i,f(\alpha^*(x_i,D),W)\} + \frac{\nu_1}{2}\|W\|^2_F$
\end{center}
Here D is not explicitly defined in the function but implicitlu in the sparse coding step ($\alpha^*(x_i,D)$ )

\subsubsection{Label Consistent K-SVD (LC-KSVD)}
\cite{6516503} add a mabem consistency regularization term and a joint classification error and label consistency regularization term into the objective function :
\begin{center}
 $<D,\alpha> = \underset{D,\alpha}{\argmin} \|X - D\alpha\|^2_2$ s.t. $ \forall i, \|\alpha_i\|_0 \leq T$
\end{center}
They refer to this two problem as LC-KSVD1 and LC-KSVD2 respectivly.

\subsubsection{LC-KSVD1}
The objective function for dictionary construction is  defined as
\begin{center}
 $<D,A,\alpha> = \underset{D,A,\alpha}{\argmin} \| X - D\alpha \|^2_2 + \mu \|Q -A\alpha\|^2_2$  s.t. $\forall i, \|\alpha_i\|_0 \leq T$
\end{center}

Where $\mu$ constrols the contribution between reconstruction and label consistency regularization. $Q = [q_1,...,q_N] \in \R^{K \times N}$ are the "discriminative" sparse codes of input signal X for classification.\\
Q is a discriminative sparse code corresponding to an input signal and the dictionary atoms, the nonzero values occur when $signal_i$ and $atom_i$ share the same label.
\begin{figure}[h]
 \centering
 \includegraphics[scale=1]{../Documentations/Q_explications.png}
 % Q_explications.png: 487x177 px, 96dpi, 12.88x4.68 cm, bb=0 0 365 133
 \caption{Example of Q, each color correspond to a class (white color  is the lack of classes)}
\end{figure}
To use K-SVD algorithm we can rewritte our previous equation:
\begin{center}
$ <D,A,\alpha> = \underset{D,A,\alpha}{\argmin} \| \begin{pmatrix} 
X  \\
\sqrt{\mu}Q
\end{pmatrix} -  \begin{pmatrix} 
D  \\
\sqrt{\mu}A
\end{pmatrix}\alpha \|^2_2 $  \\ \vspace{0.5cm} s.t. $\forall i, \|\alpha_i\|_0 \leq T $
\end{center}
We define $X_{new}  =  \begin{pmatrix} 
X  \\
\sqrt{\mu}Q
\end{pmatrix}$ and $D_{new} =  \begin{pmatrix} 
D  \\
\sqrt{\mu}A
\end{pmatrix}$. The matrix $D_{new}$ is $L_2$ normalized.We can solve our minimization problem using K-SVD algorithm on:
\begin{center}
 $<D,A,\alpha> = \underset{D,A,\alpha}{\argmin} \| X_{new} - D_{new}\alpha \|^2_2$ \\  \vspace{0.5cm}s.t. $\forall i, \|\alpha_i\|_0 \leq T$
\end{center}
But  we cannot use D and A  directly after employing K-SVD algorithm because D and A are $L_2$ normalized in $D_{new}$. The desired $\hat{D} and \hat{A}$ can be compute as follow:\vspace{0.5cm}\begin{center}
$\hat{D} = (
\frac{d_1}{\|d1\|_2}  ...  \frac{d_K}{\|d_K\|_2} 
)$   and $\hat{A} =
(\frac{a_1}{\|d_1\|_2}  ...  \frac{a_K}{\|d_K\|_2} 
)$
\end{center}

\vspace{0.5cm}
\begin{lstlisting}[language=Python,frame=single]
Input: X, Q, lambda1, K
Output: D, A
===========================================================================
D_0 = K-SVD(X,lambda1)
A_0 = Q * tranpose(X) * inv(X*tranpose(X) + lamba2 * Id)
h_0 = omp(Dnew,Xnew,lambda1)

Initialize Xnew and Dnew
D = K-SVD(Dnew,Xnew,lambda1)

Extract D, A
return D, A
\end{lstlisting}
\vspace{0.5cm}
With $A_0 = Q X^T (XX^T + \lambda_2 I)^{-1}$
\vspace{0.5cm}\\
In our case we don't want to train the classifier, we only want to find new features. This is why we only apply LC-KSVD1.\\
You can find my Python code of  LC-KSVD1 on MNIST in my Github repository: \texttt{LC-KSVD.py}
